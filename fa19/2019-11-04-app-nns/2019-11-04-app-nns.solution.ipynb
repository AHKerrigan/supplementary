{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a bit of code to make things work on Kaggle\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if os.path.exists(\"/kaggle/input/ucfai-supplementary-fa19-app-nns\"):\n",
    "    DATA_DIR = Path(\"/kaggle/input/ucfai-supplementary-fa19-app-nns\")\n",
    "else:\n",
    "    DATA_DIR = Path(\"data/\")\n",
    "\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Applications\n",
    "Today, we are going to create models for a few different datasets. This will be pretty barebones, as we are going to jump right in! You can view solutions on our github for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision import models as pretrained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torchsummary\n",
    "except:\n",
    "    torchsummary = None\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "BATCH_TEMPLATE = \"Epoch [{} / {}], Batch [{} / {}]:\"\n",
    "EPOCH_TEMPLATE = \"Epoch [{} / {}]:\"\n",
    "TEST_TEMPLATE = \"Epoch [{}] Test:\"\n",
    "\n",
    "def print_iter(curr_epoch=None, epochs=None, batch_i=None, num_batches=None, writer=None, msg=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Formats an iteration. kwargs should be a variable amount of metrics=vals\n",
    "    Optional Arguments:\n",
    "        curr_epoch(int): current epoch number (should be in range [0, epochs - 1])\n",
    "        epochs(int): total number of epochs\n",
    "        batch_i(int): current batch iteration\n",
    "        num_batches(int): total number of batches\n",
    "        writer(SummaryWriter): tensorboardX summary writer object\n",
    "        msg(bool): if true, doesn't print but returns the message string\n",
    "\n",
    "    if curr_epoch and epochs is defined, will format end of epoch iteration\n",
    "    if batch_i and num_batches is also defined, will define a batch iteration\n",
    "    if curr_epoch is only defined, defines a validation (testing) iteration\n",
    "    if none of these are defined, defines a single testing iteration\n",
    "    if writer is not defined, metrics are not saved to tensorboard\n",
    "    \"\"\"\n",
    "    if curr_epoch is not None:\n",
    "        if batch_i is not None and num_batches is not None and epochs is not None:\n",
    "            out = BATCH_TEMPLATE.format(curr_epoch + 1, epochs, batch_i, num_batches)\n",
    "        elif epochs is not None:\n",
    "            out = EPOCH_TEMPLATE.format(curr_epoch + 1, epochs)\n",
    "        else:\n",
    "            out = TEST_TEMPLATE.format(curr_epoch + 1)\n",
    "    else:\n",
    "        out = \"Testing Results:\"\n",
    "\n",
    "    floatfmt = []\n",
    "    for metric, val in kwargs.items():\n",
    "        if \"loss\" in metric or \"recall\" in metric or \"alarm\" in metric or \"prec\" in metric:\n",
    "            floatfmt.append(\".4f\")\n",
    "        elif \"accuracy\" in metric or \"acc\" in metric:\n",
    "            floatfmt.append(\".2f\")\n",
    "        else:\n",
    "            floatfmt.append(\".6f\")\n",
    "\n",
    "        if writer and curr_epoch:\n",
    "            writer.add_scalar(metric, val, curr_epoch)\n",
    "        elif writer and batch_i:\n",
    "            writer.add_scalar(metric, val, batch_i * (curr_epoch + 1))\n",
    "\n",
    "    out += \"\\n\" + tabulate(kwargs.items(), headers=[\"Metric\", \"Value\"], tablefmt='github', floatfmt=floatfmt)\n",
    "\n",
    "    if msg:\n",
    "        return out\n",
    "    print(out)\n",
    "\n",
    "def summary(model, input_dim):\n",
    "    if torchsummary is None:\n",
    "        raise(ModuleNotFoundError, \"TorchSummary was not found!\")\n",
    "    torchsummary.summary(model, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnose Pneumonia in Patients\n",
    "Our first dataset is going to be xray images of people with and without pneumonia. Our job is going to be able to correctly diagnose pneumonia in these patients. \n",
    "\n",
    "First, lets take a look at our data. We should look at the number of normal cases vs. pneumonia cases and the pictures themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = {\"train\": DATA_DIR / 'chest_xray' / 'train', \"test\": DATA_DIR / 'chest_xray' / 'test'}\n",
    "\n",
    "num_normal = 0\n",
    "num_pne = 0\n",
    "for f in folders.values():\n",
    "    num_normal += len(glob.glob(str(f / 'NORMAL' / '*')))\n",
    "    num_pne += len(glob.glob(str(f / 'PNEUMONIA' / '*')))\n",
    "\n",
    "# plot number of cases\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.bar([0, 1], [num_normal, num_pne], color=[(1, 0.5, 0), (0, 0.5, 1)])\n",
    "plt.title('Number of cases', fontsize=14)\n",
    "plt.xlabel('Case type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks([0, 1], ['Normal', 'Pneumonia'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_imgs = glob.glob(str(folders[\"train\"] / 'NORMAL' / '*'))[:5] + glob.glob(str(folders[\"train\"] / 'PNEUMONIA' / '*'))[:5]\n",
    "\n",
    "# plot some normal and pneumonia images\n",
    "f, ax = plt.subplots(2,5, figsize=(30,10))\n",
    "for i in range(10):\n",
    "    img = cv2.imread(sample_imgs[i])\n",
    "    print(img.shape)\n",
    "    ax[i//5, i%5].imshow(img, cmap='gray')\n",
    "    if i<5:\n",
    "        ax[i//5, i%5].set_title(\"Pneumonia\")\n",
    "    else:\n",
    "        ax[i//5, i%5].set_title(\"Normal\")\n",
    "    ax[i//5, i%5].axis('off')\n",
    "    ax[i//5, i%5].set_aspect('auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might see, there are much more pneumonia cases than normal, so we need to balance these classes or our model will overfit. We can subsample our data, so we only take 1500 normal and 1500 pneumonia cases, or augment our data using transforms to increase the number of each case's pictures. \n",
    "\n",
    "Lets augment the number of pictures to increase our data, we do that with our pytorch transforms. We will also use the [ImageFolder](https://pytorch.org/docs/stable/_modules/torchvision/datasets/folder.html#ImageFolder) dataset for loading our images. \n",
    "\n",
    "#### Challenge\n",
    "Write your own custom image folder dataset that loads images and **only** augments the normal pictures, not both pictures. Since there are more pneumonia pictures then normal, augmenting normal only will help even out the classes more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageFolder(Dataset):\n",
    "    \n",
    "    def __init__(self, root, transforms=None):\n",
    "        ### BEGIN SOLUTION\n",
    "        super(Dataset, self).__init__()\n",
    "        self.transforms = transforms\n",
    "        self.class_folders = sorted(glob.glob(str(root / '*')))\n",
    "        self.classes = np.arange(len(self.class_folders))\n",
    "        self.samples = []\n",
    "        for i, folder in enumerate(self.class_folders):\n",
    "            for j, file in enumerate(sorted(glob.glob(str(Path(folder) / '*')))):\n",
    "                self.samples.append((file, self.classes[i]))\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ### BEGIN SOLUTION\n",
    "        path, target = self.samples[index]\n",
    "        with open(path, 'rb') as f:\n",
    "            sample = Image.open(f)\n",
    "            sample.convert('RGB')\n",
    "\n",
    "        sample = self.transforms[target](sample)\n",
    "\n",
    "        return sample, target\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __len__(self):\n",
    "        ### BEGIN SOLUTION\n",
    "        return len(self.samples)\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (224, 224)\n",
    "num_workers = 4\n",
    "batch_size = 16\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(input_size),\n",
    "                                        transforms.ColorJitter(brightness=15, contrast=15, saturation=15),\n",
    "                                        transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                        transforms.RandomRotation(90),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "                                    \n",
    "train_dataset = ImageFolder(folders[\"train\"], transform=transform)\n",
    "test_dataset = ImageFolder(folders[\"test\"], transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, num_workers=num_workers, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, num_workers=num_workers, batch_size=batch_size)\n",
    "\n",
    "print(f\"Number of: Train Images: {len(train_dataset)}, Test Images: {len(test_dataset)}\")\n",
    "print(f\"Dataloader Sizes: Train: {len(train_dataloader)}, Test: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our model\n",
    "As we learned in our CNN lecture, we can use a custom model or a pretrained model with our own classifier layers attached. Lets try doing both and comparing the results! Feel free to use the cnn_block function or create your own custom functions. We can have one class that will either use a pretrained model or one we build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(output_dim, input_dim, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Calculates padding given in output and input dim, and parameters of the convolutional layer\n",
    "\n",
    "    Arguments should all be integers. Use this function to calculate padding for 1 dimesion at a time.\n",
    "    Output dimensions should be the same or bigger than input dimensions\n",
    "\n",
    "    Returns 0 if invalid arguments were passed, otherwise returns an int or tuple that represents the padding.\n",
    "    \"\"\"\n",
    "\n",
    "    padding = (((output_dim - 1) * stride) - input_dim + kernel_size) // 2\n",
    "\n",
    "    if padding < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return padding\n",
    "\n",
    "# can use this to help with padding calculations, or use in model directly!\n",
    "print(get_padding(224, 224, 4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_block(input_channels, output_channels, kernel_size, stride, padding):\n",
    "    layers = [nn.Conv2d(input_channels, output_channels, kernel_size, stride=stride, padding=padding)]\n",
    "    layers += [nn.BatchNorm2d(output_channels)]\n",
    "    layers += [nn.ReLU(inplace=True)]\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, use_pretrained=False, input_dim=224):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        \n",
    "        self.dim = input_dim\n",
    "        \n",
    "        if use_pretrained:\n",
    "            self.features = pretrained_models.resnet18(pretrained=True)\n",
    "            #for layer in self.features.parameters()\n",
    "            #    layer.requires_grad = False\n",
    "            \n",
    "            self.classifier = nn.Sequential(nn.Linear(1000, 1), nn.Sigmoid())\n",
    "        else:\n",
    "            self.features, out_c = self.build_layers()\n",
    "            self.classifier = self.build_classifier(out_c)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # squeeze out those features! It removes extra dimensions of 1 from shape.\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x.squeeze()).squeeze()\n",
    "    \n",
    "    \n",
    "    def build_classifier(self, in_c):\n",
    "        layers = []\n",
    "        ### BEGIN SOLUTION\n",
    "        layers += [nn.Linear(in_c, 1), nn.Sigmoid()]\n",
    "        return nn.Sequential(*layers)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    \n",
    "    def build_layers(self):\n",
    "        layers = []\n",
    "        in_c = 3\n",
    "        out_c = 64\n",
    "        dim = self.dim\n",
    "        \n",
    "        layers += cnn_block(in_c, out_c, 3, 1, get_padding(dim, dim, 4, 1))\n",
    "        in_c = out_c\n",
    "        out_c = 64\n",
    "        \n",
    "        layers += [nn.MaxPool2d(2, stride=2)]\n",
    "        dim = dim // 2\n",
    "        ### BEGIN SOLUTION\n",
    "        layers += cnn_block(in_c, out_c, 3, 1, get_padding(dim, dim, 4, 1))\n",
    "        in_c = out_c\n",
    "        out_c = 128\n",
    "\n",
    "\n",
    "        layers += cnn_block(in_c, out_c, 3, 1, 1)\n",
    "        in_c = out_c\n",
    "        out_c = 256\n",
    "\n",
    "        layers += [nn.MaxPool2d(2, stride=2)]\n",
    "        dim = dim // 2\n",
    "        \n",
    "        layers += cnn_block(in_c, out_c, 3, 1, get_padding(dim, dim, 4, 1))\n",
    "        in_c = out_c\n",
    "        out_c = 512\n",
    "\n",
    "        layers += [nn.MaxPool2d(2, stride=2)]\n",
    "        dim = dim // 2\n",
    "        \n",
    "        layers += cnn_block(in_c, out_c, 3, 1, get_padding(dim, dim, 4, 1))\n",
    "        in_c = out_c\n",
    "        out_c = 512\n",
    "\n",
    "        layers += [nn.MaxPool2d(2, stride=2)]\n",
    "        dim = dim // 2\n",
    "        \n",
    "        layers += cnn_block(in_c, out_c, 3, 1, get_padding(dim, dim, 4, 1))\n",
    "        ### END SOLUTION\n",
    "        layers += [nn.AdaptiveAvgPool2d(1)]\n",
    "        \n",
    "        return nn.Sequential(*layers), out_c\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "Since we have binary labels, we will use Binary CrossEntropy loss function. The Adam optimizer as always is an excellent optimizer to use for updating our model parameters. You can choose here to load a pretrained model or the one you have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device: {}\".format(device))\n",
    "learn_rate = 0.0001\n",
    "epochs = 20\n",
    "\n",
    "model = CNN_Model(use_pretrained=True).to(device)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=learn_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a test run through data\n",
    "# epoch of -1 is just a test run\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.float().to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, loss_const)\n",
    "\n",
    "            test_loss += loss.item()            \n",
    "            # round off decimal predictions to either 0 or 1\n",
    "            preds = torch.round(outputs)\n",
    "            total += targets.size(0)\n",
    "            # sum correct predictions\n",
    "            correct += torch.sum(preds == targets.data)\n",
    "\n",
    "        if epoch == -1:\n",
    "            print_iter(test_loss=test_loss/len(test_dataloader), val_acc=(correct / total) * 100.0)\n",
    "        else:\n",
    "            print_iter(curr_epoch=epoch, writer=writer, val_loss=test_loss/len(test_dataloader), val_acc=(correct / total) * 100.0)\n",
    "\n",
    "            return test_loss/len(test_dataloader), (correct / total) * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_step = 5\n",
    "best_loss = 0\n",
    "best_acc = 0\n",
    "\n",
    "print(\"Training Starting...\")\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.float().to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # backward\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss.item() # .item() extracts the raw loss value from the tensor object\n",
    "        \n",
    "        # round off decimal predictions to either 0 or 1\n",
    "        preds = torch.round(outputs)\n",
    "        total += targets.size(0)\n",
    "        # sum correct predictions\n",
    "        correct += torch.sum(preds == targets.data)\n",
    "\n",
    "        if i % print_step == 0:\n",
    "            print_iter(curr_epoch=e, epochs=epochs, batch_i=i, num_batches=len(train_dataloader), loss=train_loss/(i+1), acc=(correct / total) * 100.0)\n",
    "\n",
    "    print_iter(curr_epoch=e, epochs=epochs, writer=writer, loss=train_loss/len(train_dataloader), acc=(correct / total) * 100.0, time=(time.time() - epoch_start_time) / 60)\n",
    "\n",
    "    val_loss, val_acc = test(e)\n",
    "    \n",
    "    if best_acc < val_acc:\n",
    "        print('Saving Checkpoint..')\n",
    "        checkpoint_path = \"best.weights.pt\"\n",
    "        state = {'net': model.state_dict(), 'acc': val_acc}\n",
    "        torch.save(state, checkpoint_path)\n",
    "        best_acc = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cervical Cancer Risk Classification\n",
    "In this tabular dataset, medical information if given alongside a biosopy, telling whether a woman has cervical cancer or not. Use this information to build a model that can accurately predict whether a woman has cervical cancer or not. I'll leave this one up to you! But I'll provide help and get you started with loading the data. Refer to the NNs lecture if you need help.\n",
    "\n",
    "Some things to remember: Make sure to look at your data first! Split your data into X and Y, then use train_test_split to get your splits for your data. Then create a custom dataset and dataloader for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_DIR / 'cervical.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '?' with nans\n",
    "data = data.replace('?', np.nan)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.convert_objects(convert_numeric=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaNs\n",
    "Since we have NaNs all throughout the data, lets fill them instead of dropping them.\n",
    "\n",
    "For continuous variable, we fill the median value for that column.\n",
    "For categorical variable, we fill with a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Number of sexual partners'] = data['Number of sexual partners'].fillna(data['Number of sexual partners'].median())\n",
    "data['Smokes'] = data['Smokes'].fillna(1)\n",
    "# Fill the rest here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
