{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "nb-title"
    ],
    "title": "Introduction to Statistics, Featuring Datascience"
   },
   "source": [
    "<img src=\"https://ucfai.org/groups/supplementary/sp20/02-06-stats-intro/stats-intro/banner.png\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <span class=\"btn btn-success btn-block\">\n",
    "        Meeting in-person? Have you signed in?\n",
    "    </span>\n",
    "</div>\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Introduction to Statistics, Featuring Datascience </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <strong> None</strong>\n",
    "        (<a href=\"https://github.com/calvinyong\">@calvinyong</a>)\n",
    "    \n",
    "        <strong> None</strong>\n",
    "        (<a href=\"https://github.com/jordanstarkey95\">@jordanstarkey95</a>)\n",
    "     on 2020-02-06</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2I0tr2O8I4To"
   },
   "source": [
    "## Purpose\n",
    "\n",
    "The goal of this workshop is to provide the essential statistical knowledge required for data science.\n",
    "\n",
    "To demonstrate these essentials, we'll look at a \n",
    "\n",
    "This workshop assumes you have reviewed the supplementary [Python3 workshop](https://ucfai.org/supplementary/sp20/math-primer-python-bootcamp) and core [Linear Regression workshop](https://ucfai.org/core/sp20/linear-regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDn-mJ-fOag3"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Lets look at how statistical methods are used in an applied machine learning project:\n",
    "\n",
    "* Problem Framing: Requires the use of exploratory data analysis and data mining.\n",
    "* Data Understanding: Requires the use of summary statistics and data visualization.\n",
    "* Data Cleaning: Requires the use of outlier detection, imputation and more.\n",
    "* Data Selection: Requires the use of data sampling and feature selection methods.\n",
    "* Data Preparation: Requires the use of data transforms, scaling, encoding and much more.\n",
    "* Model Evaluation: Requires experimental design and resampling methods.\n",
    "* Model Configuration: Requires the use of statistical hypothesis tests and estimation statistics.\n",
    "* Model Selection: Requires the use of statistical hypothesis tests and estimation statistics.\n",
    "* Model Presentation: Requires the use of estimation statistics such as confidence intervals.\n",
    "* Model Predictions: Requires the use of estimation statistics such as prediction intervals\n",
    "\n",
    "[Source: https://machinelearningmastery.com/statistics_for_machine_learning/]\n",
    "\n",
    "## Descriptive and Inferential Statistics\n",
    "\n",
    "**Descriptive statistics** identify patterns in the data, but they don't allow for making hypotheses about the data.\n",
    "\n",
    "Within descriptive statistics, there are three measures used to describe the data: *central tendency* and *deviation*. \n",
    "\n",
    "* Central tendency tells you about the centers of the data. Useful measures include the mean, median, and mode.\n",
    "* Variability tells you about the spread of the data. Useful measures include variance and standard deviation.\n",
    "* Correlation or joint variability tells you about the relation between a pair of variables in a dataset. Useful measures include covariance and the correlation coefficient.\n",
    "\n",
    "**Inferential statistics** allow us to make hypotheses (or inferences) about a sample that can be applied to the population. \n",
    "\n",
    "In statistics, the **population** is a set of all elements or items that you’re interested in. Populations are often vast, which makes them inappropriate for collecting and analyzing data. That’s why statisticians usually try to make some conclusions about a population by choosing and examining a representative subset of that population.\n",
    "\n",
    "This subset of a population is called a **sample**. Ideally, the sample should preserve the essential statistical features of the population to a satisfactory extent. That way, you’ll be able to use the sample to glean conclusions about the population.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qznYU5DPr8Sw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWCvfp0FRCgK"
   },
   "outputs": [],
   "source": [
    "## Load the Boston dataset into a variable called boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjj-Nayyns90"
   },
   "outputs": [],
   "source": [
    "## Separate the features from the target\n",
    "x = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wMM4LTc_oVyg"
   },
   "source": [
    "To view the dataset in a standard tabular format with the all the feature names, you will convert this into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4F61rjQoMGt"
   },
   "outputs": [],
   "source": [
    "## Take the columns separately in a variable\n",
    "columns = boston.feature_names\n",
    "\n",
    "## Create the Pandas dataframe from the sklearn dataset\n",
    "boston_df = pd.DataFrame(boston.data)\n",
    "boston_df.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BAhSo6UKeoda"
   },
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "This portion serves as a very basic primer on Descriptive statistics and will explain concepts which are fundamental to understanding Inferential Statistics, its tools and techniques. We will be using Boston House Price dataset: \n",
    "\n",
    "https://www.kaggle.com/c/boston-housing\n",
    "\n",
    "Here is the Dataset description: \n",
    "\n",
    "* crim\n",
    " * per capita crime rate by town.\n",
    "\n",
    "* zn\n",
    " * proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "* indus\n",
    " * proportion of non-retail business acres per town.\n",
    "\n",
    "* chas\n",
    " * Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "* nox\n",
    " * nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "* rm\n",
    " * average number of rooms per dwelling.\n",
    "\n",
    "* age\n",
    " * proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "* dis\n",
    " * weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "* rad\n",
    " * index of accessibility to radial highways.\n",
    "\n",
    "* tax\n",
    " * full-value property-tax rate per \\$10,000.\n",
    "\n",
    "* ptratio\n",
    " * pupil-teacher ratio by town.\n",
    "\n",
    "* black\n",
    " * 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "* lstat\n",
    " * lower status of the population (percent).\n",
    "\n",
    "* medv\n",
    " * median value of owner-occupied homes in \\$1000s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSdGrw5v5HYj"
   },
   "source": [
    "### Summary Statistics\n",
    "\n",
    "To begin learning about the sample, we uses pandas' `describe` method, as seen below. The column headers in bold text represent the variables we will be exploring. Each row header represents a descriptive statistic about the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "-cQGavhHLToe",
    "outputId": "f8799241-a3a3-445d-ab4c-a174336c12d0"
   },
   "outputs": [],
   "source": [
    "boston_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rY-1_m_fNKhn"
   },
   "source": [
    "`describe` isnt particularly enlightening on the distributions of our data \n",
    "but can help use figure out how to approach our visualization techniques. Before we explore essential graphs for exploring our data, lets use a few more important pandas methods to aid in our exploratory data analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "zeqTT3AgQ9CG",
    "outputId": "1c08537b-4f7b-489a-d355-3431171c9668"
   },
   "outputs": [],
   "source": [
    "print (\"Rows     : \" , boston_df.shape[0])\n",
    "print (\"Columns  : \" , boston_df.shape[1])\n",
    "print (\"\\nFeatures : \\n\" , boston_df.columns.tolist())\n",
    "print (\"\\nMissing values :  \", boston_df.isnull().sum().values.sum())\n",
    "print (\"\\nUnique values :  \\n\",boston_df.nunique())\n",
    "print('\\n')\n",
    "print(boston_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ANj7maGFNloE"
   },
   "source": [
    "We first show the shape of our dataset. We have 506 rows for our 13 features (columns). This is a relatively nice dataset in that there arent many missing values. A future supplementary lecture in preprocessing will cover techniques in dealing with missing values. \n",
    "We can see that there is a feature (CHAS) which has 2 unique values. This could indicate that it is a catgeorical variables. There are three types of statistical data we may be dealing with: \n",
    "\n",
    "* Numerical (Quantitative) data have meaning as a measurement, such as a person’s height, weight, IQ, or blood pressure; or they’re a count, such as the number of stock shares a person owns or how many teeth a dog has. Numerical data can be further broken into two types: discrete and continuous.\n",
    "\n",
    " * Discrete data represent items that can be counted; they take on possible values that can be listed out. The list of possible values may be fixed (also called finite); or it may go from 0, 1, 2, on to infinity (making it countably infinite). For example, the number of heads in 100 coin flips takes on values from 0 through 100 (finite case), but the number of flips needed to get 100 heads takes on values from 100 (the fastest scenario) on up to infinity (if you never get to that 100th heads).  \n",
    "\n",
    " * Continuous data represent measurements; their possible values cannot be counted and can only be described using intervals on the real number line. For example, the exact amount of gas purchased at the pump for cars with 20-gallon tanks would be continuous data from 0 gallons to 20 gallons, represented by the interval [0, 20], inclusive. Continuous data can be thought of as being uncountably infinite. \n",
    "\n",
    "* Categorical (Qualitative) data represent characteristics such as a person’s gender, marital status, hometown, or the types of movies they like. Categorical data can take on numerical values (such as “1” indicating married and “2” indicating unmarried), but those numbers don’t have mathematical meaning. The process of giving these mathematical meaning for our model to understand is variable encoding. This will be covered in the preprocessing supplementary lecture.\n",
    "\n",
    "* Ordinal data mixes numerical and categorical data. The data fall into categories, but the numbers placed on the categories have meaning. For example, rating a restaurant on a scale from 0 (lowest) to 4 (highest) stars gives ordinal data. Ordinal data are often treated as categorical, where the groups are ordered when graphs and charts are made. However, unlike categorical data, the numbers do have mathematical meaning. For example, if you survey 100 people and ask them to rate a restaurant on a scale from 0 to 4, taking the average of the 100 responses will have meaning. This would not be the case with categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYGA40Vs5WCV"
   },
   "source": [
    "### Central Tendencies\n",
    "\n",
    "The central tendencies are values which represent the central or 'typical' value of the given distribution. The three most popular central tendency estimates are the mean, median and mode. Typically, in most cases, we resort to using mean (for normal distributions) and median (for skewed distributions) to report central tendency values.\n",
    "\n",
    "A good rule of thumb is to use mean when outliers don't affect its value and median when it does (Bill Gates joke, anyone?).\n",
    "\n",
    "Calculating the mean and median are extremely trivial with Pandas. In the following cell, we have calculated the mean and median of the average number of rooms per dwelling.  As we can see below, the mean and the median are almost equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QdScfhKT5hi6",
    "outputId": "c1984c7b-a7db-462d-d727-c9dcfed7bd94"
   },
   "outputs": [],
   "source": [
    "rooms = boston_df['RM']\n",
    "rooms.mean(), rooms.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_qYx_3U05-BQ"
   },
   "source": [
    "If the mean, median and the mode of a set of numbers are equal, it means, the distribution is symmetric. The more skewed is the distribution, greater is the difference between the median and mean, and we should lay greater emphasis on using the median as opposed to the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDrDJ4Q26DnE"
   },
   "source": [
    "### Measures of Spread\n",
    "\n",
    "Apart from the central or typical value of the data, we are also interested in knowing how much the data spreads. That is, how far from the mean do values tend to go. Statistics equips us with two measures to quantitatively represent the spread: the variance and the standard deviation. They are dependent quantities, with the standard deviation being defined as the square root of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ajo-qqwn6IJY",
    "outputId": "c30c7935-145d-4d8e-cef0-b29a18496d8e"
   },
   "outputs": [],
   "source": [
    "rooms.std(), rooms.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9nRX1UwY6PpQ"
   },
   "source": [
    "The mean and the standard deviation are often the best quantities to summarize the data for distributions with symmetrical histograms without too many outliers. As we can see from the histogram below, this indeed is the case for RM feature. Therefore, the mean and the standard deviation measures are sufficient information and other tendencies such as the median does not add too much of extra information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "dLz3YpNM6WjW",
    "outputId": "02f9faa2-961c-4d4f-8d2d-7d23ab4b83ae"
   },
   "outputs": [],
   "source": [
    "sns.distplot(rooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_jEDxf4FEm7"
   },
   "source": [
    "This is an example of a normal (Gaussian) distribution. It is ideal that our continuous variables folllow this distribution because of the central limit theorem. See [here](https://towardsdatascience.com/why-data-scientists-love-gaussian-6e7a7b726859) for an explanation on why the Gaussian is ideal for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o3WtpCJHlcOo",
    "outputId": "aa12fa6d-c9e9-4502-c43d-13e6f269aa43"
   },
   "outputs": [],
   "source": [
    "stats.normaltest(rooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUEN9xHUlspl"
   },
   "source": [
    "`normaltest` returns a 2-tuple of the chi-squared statistic, and the associated p-value. Given the null hypothesis that x came from a normal distribution, the p-value represents the probability that a chi-squared statistic that large (or larger) would be seen. If the p-val is very small, it means it is unlikely that the data came from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5twMF7a7H9p"
   },
   "source": [
    "Here is an example of a skewed dsitribution and how to fix it in order to fit a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "nFbLmQkJE568",
    "outputId": "d27251b3-d4a2-4b47-ac20-fa934bda4a48"
   },
   "outputs": [],
   "source": [
    "age = boston_df['AGE']\n",
    "print(age.std(), age.mean())\n",
    "sns.distplot(age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOEvdcTQFhv8"
   },
   "source": [
    "There are many ways to transform skewed data in order to fit a normal distribution. This will transform the data into a normal distribution. Moreover, you can also try Box-Cox transformation which calculates the best power transformation of the data that reduces skewness although a simpler approach which can work in most cases would be applying the natural logarithm. More details about Box-Cox transformation can be found here and here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "etS0SMgPjpL7",
    "outputId": "b57dbbed-f862-4d2c-f64f-c9a36031f58c"
   },
   "outputs": [],
   "source": [
    "log_age = np.log(age)\n",
    "print(log_age.std(), log_age.mean())\n",
    "sns.distplot(log_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpL_KykGkl69"
   },
   "source": [
    "Although there is a long left tail, the log transformation reduces the deviation of the data. Can we measure normalcy? Yes! Rather than read from a Histogram, we can perform the Normal Test. This comes in the Scipy package and that lets us calculate the probability that the distrbution is normal, by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A25Tee1hqOhF"
   },
   "source": [
    "### Univariate Analysis\n",
    "\n",
    "It is a common practice to start with univariate outlier analysis where you consider just one feature at a time. Often, a simple box-plot of a particular feature can give you good starting point. You will make a box-plot using `seaborn` and you will use the `DIS` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "i3-yzoVcr0bK",
    "outputId": "b1fe3257-e791-4fd8-b1b7-74da909a2987"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=boston_df['DIS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qtoOFrOtv6v"
   },
   "source": [
    " A box-and-whisker plot is helpful for visualizing the distribution of the data from the mean. Understanding the distribution allows us to understand how far spread out her data is from the mean. Check out [how to read and use a Box-and-Whisker plot](https://flowingdata.com/2008/02/15/how-to-read-and-use-a-box-and-whisker-plot/).\n",
    "\n",
    "\n",
    "The above plot shows three points between 10 to 12, these are **outliers** as they're are not included in the box of other observations. Here you analyzed univariate outlier, i.e., you used DIS feature only to check for the outliers.\n",
    "\n",
    "An outlier is considered an observation that appears to deviate from other observations in the sample. We can spot outliers in plots like this or scatterplots.\n",
    "\n",
    "Many machine learning algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in input data can skew and mislead the training process of machine learning algorithms resulting in longer training times and less accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rOemy8_OvEZ1"
   },
   "source": [
    "A more robust way of statistically identifying outliers is by using the Z-Score.\n",
    "\n",
    "The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. [*Source definition*](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/).\n",
    "\n",
    "The idea behind Z-score is to describe any data point regarding their relationship with the Standard Deviation and Mean for the group of data points. Z-score is about finding the distribution of data where the mean is 0, and the standard deviation is 1, i.e., normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "BMjhIRUaxSxW",
    "outputId": "e3722a6a-6a16-408c-f222-749f7ada4d7a"
   },
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(boston_df))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "B82oMciUxcYo",
    "outputId": "08aaf81b-2a38-4624-edb7-a2942970a154"
   },
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "## The first array contains the list of row numbers and the second array contains their respective column numbers.\n",
    "print(np.where(z > 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1RiKaBexhON"
   },
   "source": [
    "You could use Z-Score and set its threshold to detect potential outliers in the data. With this, we can remove the outliers from our dataframe. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jUUPNOiX3SiU",
    "outputId": "75e0c93f-71dc-44ae-b5d3-944eae5a4807"
   },
   "outputs": [],
   "source": [
    "print(boston_df.shape)\n",
    "boston_df = boston_df[(np.abs(stats.zscore(boston_df)) < 3).all(axis=1)]\n",
    "print(boston_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zklYjDQe3kJf"
   },
   "source": [
    "For each column, first it computes the Z-score of each value in the column, relative to the column mean and standard deviation.\n",
    "Then is takes the absolute of Z-score because the direction does not matter, only if it is below the threshold.\n",
    "all(axis=1) ensures that for each row, all column satisfy the constraint.\n",
    "Finally, result of this condition is used to index the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkBGJ7m5qVvT"
   },
   "source": [
    "## References \n",
    "\n",
    "* https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781784390150/2\n",
    "\n",
    "* https://www.learndatasci.com/tutorials/data-science-statistics-using-python/\n",
    "\n",
    "* https://www.datacamp.com/community/tutorials/demystifying-crucial-statistics-python\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "descriptive-stats-workshop.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
